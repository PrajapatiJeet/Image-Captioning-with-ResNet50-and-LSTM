# Image Captioning with ResNet50 and LSTM

This project implements a deep learning model to automatically generate descriptive captions for images. It uses a classic **Encoder-Decoder architecture**, leveraging a pre-trained Convolutional Neural Network (CNN) to understand image content and a Recurrent Neural Network (RNN) to generate descriptive text.

*An example of a caption generated by the final model.*

---

## Table of Contents
- [Model Architecture](#model-architecture)
- [Dataset](#dataset)
- [Project Pipeline](#project-pipeline)
- [Setup and Usage](#setup-and-usage)
- [Results and Conclusion](#results-and-conclusion)
- [Future Improvements](#future-improvements)

---

## Model Architecture

The model is composed of two main parts that work together to generate captions:

* **Encoder (Image Feature Extractor)**: A pre-trained **ResNet50** model, with its final classification layer removed, is used as the feature extractor. It processes an input image and outputs a compact 2048-element vector that serves as a numerical representation of the image's content.

* **Decoder (Text Generator)**: A Recurrent Neural Network, specifically a **Long Short-Term Memory (LSTM)** network, takes the image feature vector from the encoder. It then generates the caption word by word, learning the sequence and structure of language.

The two branches are merged before a final output layer predicts the next word in the sequence.

---

## Dataset

This model was trained on the **Flickr8k dataset**, which contains 8,000 images, each paired with five different human-written captions.

* **Dataset Source**: [Flickr8k Dataset on Kaggle](https://www.kaggle.com/datasets/adityajn105/flickr8k)

---

## Project Pipeline

The project is structured into three main stages, all contained within the Jupyter Notebook:

1.  **Data Preparation**:
    * Captions are loaded and cleaned by converting them to lowercase, removing punctuation, and creating a vocabulary of all unique words.
    * Image features are extracted for every image using the ResNet50 encoder. These features are saved to a file (`features.pkl`) to avoid re-computation during training.

2.  **Model Training**:
    * The cleaned captions and image features are loaded.
    * A custom data generator is used to feed batches of data to the model efficiently.
    * The CNN-RNN model is trained for 40 epochs, with the training loss showing a consistent decrease, indicating successful learning.

3.  **Inference and Caption Generation**:
    * The trained model (`image_captioning_model.h5`) and tokenizer are loaded.
    * A **Beam Search** algorithm is implemented to generate captions for new, unseen images. This method explores multiple potential sentence paths to produce more coherent and contextually relevant results than a simple greedy search.

---

## Setup and Usage

To run this project, you can use the provided `image-captioning.ipynb` notebook in an environment like Kaggle or Google Colab, or set it up locally.

### Dependencies

The project requires the following Python libraries:

* TensorFlow/Keras
* NumPy
* Matplotlib
* Pillow
* NLTK
* KerasTuner

You can install them using pip:

```bash
pip install tensorflow numpy matplotlib pillow nltk keras-tuner
 ```
### Running the Notebook
* **Data Preparation**: Run the first code cell to process the captions and extract image features.
* **Model Training**: Run the subsequent cells to train the model, perform hyperparameter tuning, and then train the final, optimized model.
* **Inference**: Run the final cell to load the fully trained model and generate a caption for a sample image.

---
## Results and Conclusion
This project successfully implemented a complete deep learning pipeline to generate captions for images using a ResNet50 encoder and an LSTM decoder.

### Positive Outcomes üëç
* **Successful Implementation**: The end-to-end architecture, from feature extraction to training, tuning, and inference, was built and executed successfully.
* **Model Learning**: The training logs consistently showed a decreasing loss, confirming that the model was effectively learning the relationship between image features and text sequences.
* **Concept Recognition**: After extended training, the model began to correctly identify key objects in the images, such as recognizing a "child" where it previously did not.
* **Improved Coherence**: Using Beam Search for inference resulted in more grammatically structured and coherent sentences compared to a simple greedy search.

### Challenges and Limitations üëé
* **Model Hallucination**: The primary challenge was the model "hallucinating" objects and actions not present in the image (e.g., describing a child on a "unicycle"). This is a classic symptom of a model trained on a limited dataset.
* **Dataset Size**: The Flickr8k dataset, while good for learning and prototyping, is ultimately too small for this complex task, leading to a lack of contextual understanding and occasional repetition in the generated captions.

---
## Future Improvements
While training for more epochs helps, significant improvements can be achieved through more fundamental changes:

1.  **Use a Larger Dataset**: This is the single most impactful change. Training the same architecture on a larger dataset like **MS COCO** (which has over 330,000 images) would expose the model to far more contextual examples and dramatically reduce hallucination.
2.  **Upgrade the Model Architecture**:
    * **Encoder (Image Understanding)**: Instead of ResNet50, using a more modern and powerful pre-trained CNN like **EfficientNet** or a **Vision Transformer (ViT)** can provide the decoder with a much richer understanding of the image content.
    * **Decoder (Text Generation)**: The LSTM is effective, but state-of-the-art results are now achieved using **Transformer-based decoders**. A full Transformer architecture (like the one used in models like GPT) has a more sophisticated attention mechanism that is better at tracking long-range dependencies in a sentence, leading to more logical and less repetitive captions.
3.  **Refine Hyperparameters**: As we did with KerasTuner, a more exhaustive search for the optimal embedding size, LSTM units, and dropout rate can further fine-tune the model's learning capacity.
